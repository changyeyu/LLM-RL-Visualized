<h1 align="center"> AI-Roadmap </h1>

<p align="center">
  <img src="images_chinese/source_svg/AI%20Roadmap(AI%E7%9F%A5%E8%AF%86%E6%9E%B6%E6%9E%84).svg" alt="AI Roadmap（AI知识架构）" />
</p>

<br>

---

<br>

<h1 align="center">AI 学习资料 · 精品力荐</h1>

| 资料 | 岗位/方向 | 说明 |
| --- | --- | --- |
| Python 官方教程（中文版） 🌐[WEB](https://docs.python.org/zh-cn/3.13/tutorial/index.html) | AI 算法 & 工程 | 另外结合 NumPy、pandas 等外部组件进行实践 |
| 《动手学深度学习（PyTorch 版）》· 李沐｜[豆瓣](https://book.douban.com/subject/36142067/)｜📕[官方PDF](https://zh-v2.d2l.ai/d2l-zh-pytorch.pdf) | AI 算法 & 工程 | 豆瓣 9.4 分，理论与实践合一 |
| 《神经网络与深度学习》· 邱锡鹏｜[豆瓣](https://book.douban.com/subject/35044046/) | AI 算法 & 工程 | 豆瓣 9.4 分，内容综合 |
| 《机器学习》· 李宏毅｜📺[视频](https://speech.ee.ntu.edu.tw/~hylee/ml/2025-spring.php) | AI 算法 & 工程 | 台大课程，体系完整，入门必备 |
| 《强化学习（第 2 版）》｜[豆瓣](https://book.douban.com/subject/34809689/)｜📕[官方PDF](http://incompleteideas.net/book/RLbook2020.pdf) | AI 算法 | RL之父出品，基础理论扎实（但缺少 PPO ） |
| UC Berkeley 深度强化学习教程｜📕[课程主页](https://rail.eecs.berkeley.edu/deeprlcourse/) | AI 算法 | Berkeley 在 RL 领域建树颇多，内容严谨 |
| 《普林斯顿微积分读本（修订版）》｜[豆瓣](https://book.douban.com/subject/26899701/) | AI 算法 | 豆瓣 9.5 分，知识体系、易读 |
| 《线性代数及其应用（原书第 5 版）》｜[豆瓣](https://book.douban.com/subject/30310517/) | AI 算法 | 豆瓣 9.7 分，经典教材 |
| 《普林斯顿概率论读本》｜[豆瓣](https://book.douban.com/subject/35193606/) | AI 算法 | 豆瓣 9.2 分，系统全面 |
| 《统计学习方法（第 2 版）》· 李航｜[豆瓣](https://book.douban.com/subject/33437381/) | AI 算法 | 豆瓣 9.3 分，传统机器学习基础扎实 |
| 《最优化导论（第 4 版）》｜[豆瓣](https://book.douban.com/subject/26732914/) | AI 算法 | 豆瓣 9.1 分，体系清晰 |
| 《信息论基础》｜[豆瓣](https://book.douban.com/subject/2305237/) | AI 算法 | 豆瓣 9.0 分；重点理解“熵” |
| 宋浩老师 · 各类数学课程｜📺[B 站](https://space.bilibili.com/66607740) | AI 算法 | 各类数学课程、覆盖面广 |
| 《深度学习推荐系统 2.0》· 王喆｜[豆瓣](https://book.douban.com/subject/37286473/) | 推荐算法 | 第一/二版口碑俱佳，搜广推入门力荐 |
| 《大模型算法：强化学习、微调与对齐》｜[豆瓣](https://book.douban.com/subject/37331056/) | 大模型算法 | 豆瓣 9.4 分；算法工程师出品，50% LLM + 50% RL |
| 国内外各家 LLM / VLM 汇总｜[LLM-VLM-index（汇总）.md](https://github.com/changyeyu/LLM-RL-Visualized/blob/master/LLM-VLM-index%20(%E6%B1%87%E6%80%BB).md) | 大模型算法 | Paper / 代码 / 配置集中整理 |
| 《视觉 SLAM 十四讲（第 2 版）》｜[豆瓣](https://book.douban.com/subject/34782244/)｜📕[Repo/PDF](https://github.com/gaoxiang12/slambook2) | SLAM 算法 | 豆瓣 9.3 分，配套开源 |
| CUDA C++ Programming Guide｜📕[PDF](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf) | AI 工程 | NVIDIA 官方教程，理解 GPU & CUDA 架构必读，官方长期更新 |
| 论文  | AI 算法 | 多数算法只能看论文了解：https://arxiv.org/  大多均可在此查阅下载。 |

<br>

---

<br>

<h1 align="center">Q&A</h1>

## 数学好难，需要学吗？
- 只想**基础了解**：可在豆瓣检索“图解深度学习***”等入门读物，或在 B 站搜索 AI 科普视频。
- 面向**AI 工程岗位**：先看 AI 书籍，遇到不懂的数学概念再查相关数学资料。
- 面向**AI 算法 / 大模型算法 / 推荐算法**：建议系统学习各类数学、长期坚持，效率远高于零碎搜索。数学基础打好后，强化学习等更易理解。
- 上述数学资料多为国外教材，体系化、脉络清晰、推导详尽，通常比常见本科教材更友好，相信很多人看了之后会真的喜欢上这些数学，而不纯粹是为了AI去阅读。

## 应聘 AI 技术岗，还需要学习哪些知识/技巧？
- **实操**：基于 PyTorch 写 Demo、训练实用模型；积累超参数调优、数据与样本构建、模型落地经验。
- **看论文**：AI 发展快，书籍难覆盖新算法。基础打牢后需长期坚持、追踪前沿论文与技术分享（如 GPT 早期论文、最新各大厂技术报告；见我的[汇总](https://github.com/changyeyu/LLM-RL-Visualized/blob/master/LLM-VLM-index%20(%E6%B1%87%E6%80%BB).md)）。

## 如何看论文？
- **主动查阅**：arXiv 汇集大多数 AI 论文，可自由检索、免费下载 PDF：<https://arxiv.org/>
- **被动关注**：关注“机器之心”“新智元”等公众号、知乎专栏。它们会筛选热门论文与开源项目，但也可能夹杂噱头——务必独立判断。
- **批判性阅读**：部分论文实验与结论可信度难辨，更重要的是对自身想法的启发。
- **优先级**：优先看大厂核心团队、DeepMind、OpenAI 等机构论文；优先看代码 / 模型敢与论文一起开源的工作。

## 关于模型加速与大模型 Infra
- 该方向需求增长：需要 **AI 基础理论 + 传统后端工程 + GPU 架构** 知识。
- 大模型因集群规模与成本，对 **GPU、加速、通信** 要求极高，工程优化价值巨大。DeepSeek 的火爆，工程优化（算子、并行与通信）是关键因素之一。
- GPU 的线程/调度/存储/通信与传统冯·诺依曼体系差异大。上文《CUDA C++ Programming Guide》适合入门，但还不够。
- 建议深入主流框架与组件：DeepSpeed（ZeRO）、Megatron、NCCL 通信、PyTorch FSDP、FlashAttention、vLLM，以及 DeepSeek 的 DeepEP 等。

## 关于强化学习（RL）
- RL 越来越重要：AI发展到今天，仅靠人类数据已不足以作为 Teacher Label，必须打开人类的天窗，让AI在外面的世界里自由探索、自由进化。尤其是结合 Agent 的 RL 将持续提升大模型能力。
- **RL 的应用场景**：大模型训练、具身智能控制、金融量化、游戏 AI 等。
- **RL 的挑战**：RL 训练常常极其不稳定，因为模型很狡猾，人类常常被“忽悠”（Reward Hacking）。所以，RL 的最大难点在于：在人类能力有限的前提下，如何设计**可靠、客观的 Reward** 来持续驱动模型进化。好在最近 RL 越来越靠谱。笔者是国内较早一批从事 LLM + RL 的工程师，彼时国内外都主要以 PPO 做 RLHF，TRL等RL框架也有各种问题，几乎没有别的选择，社区内弥漫各种诡异的问题反馈 —— **Loss 斗转星移与孤鹜齐飞，Reward 梦断蓝桥共长天一色**。如今RLVR、GRPO 的流行，不得不说是 RLer 的一大福利！

## 如何学习 RL？
- **理论**：RL 涉及面广、对数学要求高。先夯实数学，再读 RL 经典教材与最新论文（TRPO、PPO、GRPO 等），对齐业界认知。
- **避坑**：一些书籍、PDF常常存在推导或概念错误。建议优先围绕 RL 之父教材、UC Berkeley、OpenAI 的 PPO 论文等权威来源学习。通俗易懂和严谨准确往往是鱼和熊掌的关系。
- **实操**：可基于 veRL 训练大模型，或用 RLlib + Gym 复现实验（经典控制等）。
- **资料**：我整理过一份思维导图，可下载查看——[强化学习算法图谱（rl-algo-map）.pdf](https://github.com/changyeyu/LLM-RL-Visualized/blob/master/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%9B%BE%E8%B0%B1%20(rl-algo-map).pdf)

<br>

---

<div align="center">

长期更新，欢迎点击 [Star⭐](https://github.com/changyeyu/LLM-RL-Visualized) —— 感谢鼓励！✨

Continuously updated. Click [Star⭐](https://github.com/changyeyu/LLM-RL-Visualized) to show support — thank you! ✨

</div>
