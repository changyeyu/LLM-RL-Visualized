<p align="center">
  <img src="../src/assets/banner/å¹»ç¯ç‰‡1.SVG" alt="Illustrated Large Model Algorithms: LLM, RL, VLM ..." />
</p>


<p align="center">
    <a href="./README_EN.md">
    <img
      alt="English Version"
      src="https://img.shields.io/badge/English-Version-blue?style=for-the-badge"
      width="250"
      height="50"
    />
  </a> 
  &nbsp; &nbsp;&nbsp;

  <a href="../README.md">
    <img
      alt="Chinese ä¸­æ–‡ç‰ˆæœ¬"
      src="https://img.shields.io/badge/Chinese-ä¸­æ–‡ç‰ˆæœ¬-red?style=for-the-badge"
      width="250"
      height="50"
    />
  </a>    

</p>




---
## Description

ğŸ‰ **100+ original diagrams**visualizing core concepts of large model algorithms â€” from LLMs, VLMs, and training methods (RL, RLHF, GRPO, DPO, SFT, distillation) to RAG and performance optimization.

ğŸ‰ Originally based on the diagrams from the Chinese **book** [ã€Šå¤§æ¨¡å‹ç®—æ³•ï¼šå¼ºåŒ–å­¦ä¹ ã€å¾®è°ƒä¸å¯¹é½ã€‹](https://item.jd.com/15017130.html) (<em>Large Model Algorithms: Reinforcement Learning, Fine-Tuning, and Alignment</em>), this project has since been continuously expanded with new content and improvements.

ğŸ‰ **Continuously updated** and actively maintained â€” click **Star â­** to stay tuned!

ğŸ‰ Contributions welcome â€” see the <a href="#contributing" style="color:rgb(44, 46, 49); text-decoration: underline;">Contributing Guide</a> to get involved.

Click on the images to view high-resolution versions, or browse the `.svg` vector files in the repository for infinite zoom support.


## Table of Contents
- [Overall Architecture of Large Model Algorithms (Focusing on LLMs and VLMs)](#header-1)
- [ã€LLM basicsã€‘LLM overview](#header-2)
- [ã€LLM basicsã€‘LLM structure](#header-3)
- [ã€LLM basicsã€‘LLM generation and decoding](#header-4)
- [ã€LLM basicsã€‘LLM Input](#header-5)
- [ã€LLM basicsã€‘LLM output](#header-6)
- [ã€LLM basicsã€‘MLLM and VLM](#header-7)
- [ã€LLM basicsã€‘LLM training process](#header-8)
- [ã€SFTã€‘Categories of fine-tuning techniques](#header-9)
- [ã€SFTã€‘LoRA(1 of 2)](#header-10)
- [ã€SFTã€‘LoRA(2 of 2)](#header-11)
- [ã€SFTã€‘Prefix-Tuning](#header-12)
- [ã€SFTã€‘Token ID and Token](#header-13)
- [ã€SFTã€‘Loss of SFT(cross-entropy)](#header-14)
- [ã€SFTã€‘Packing of multiple pieces of sample](#header-15)
- [ã€DPOã€‘RLHF vs DPO](#header-16)
- [ã€DPOã€‘DPO(Direct Preference Optimization)](#header-17)
- [ã€DPOã€‘Overview of DPO training](#header-18)
- [ã€DPOã€‘Impact of the Î² parameter on DPO](#header-19)
- [ã€DPOã€‘Effect of implicit reward differences on the magnitude of parameter updates](#header-20)
- [ã€Optimization without trainingã€‘Comparison of CoT and traditional Q&A](#header-21)
- [ã€Optimization without trainingã€‘CoTã€Self-consistency CoTã€ToTã€GoT <sup>[<a href="./references.md">87</a>]</sup>](#header-22)
- [ã€Optimization without trainingã€‘Exhaustive Search](#header-23)
- [ã€Optimization without trainingã€‘Greedy Search](#header-24)
- [ã€Optimization without trainingã€‘Beam Search](#header-25)
- [ã€Optimization without trainingã€‘Multinomial Sampling](#header-26)
- [ã€Optimization without trainingã€‘Top-K Sampling](#header-27)
- [ã€Optimization without trainingã€‘Top-P Sampling](#header-28)
- [ã€Optimization without trainingã€‘RAG(Retrieval-Augmented Generation)](#header-29)
- [ã€Optimization without trainingã€‘Function Calling](#header-30)
- [ã€RL basicsã€‘History of RL](#header-31)
- [ã€RL basicsã€‘Three major machine learning paradigms](#header-32)
- [ã€RL basicsã€‘Basic architecture of RL](#header-33)
- [ã€RL basicsã€‘Fundamental Concepts of RL](#header-34)
- [ã€RL basicsã€‘Markov Chain vs MDP](#header-35)
- [ã€RL basicsã€‘Using dynamic Îµ values under the Îµ-greedy strategy](#header-36)
- [ã€RL basicsã€‘Comparison of RL training paradigms](#header-37)
- [ã€RL basicsã€‘Classification of RL](#header-38)
- [ã€RL basicsã€‘Return(cumulative reward)](#header-39)
- [ã€RL basicsã€‘Backwards iteration and computation of return G](#header-40)
- [ã€RL basicsã€‘Reward, Return, and Value](#header-41)
- [ã€RL basicsã€‘QÏ€ and VÏ€](#header-42)
- [ã€RL basicsã€‘Estimate the value through Monte Carlo(MC)](#header-43)
- [ã€RL basicsã€‘TD target and TD error](#header-44)
- [ã€RL basicsã€‘TD(0), n-step TD, and MC](#header-45)
- [ã€RL basicsã€‘Characteristics of MC and TD methods](#header-46)
- [ã€RL basicsã€‘MC, TD, DP, and exhaustive search <sup>[<a href="./references.md">32</a>]</sup>](#header-47)
- [ã€RL basicsã€‘DQN model with two input-output structures](#header-48)
- [ã€RL basicsã€‘How to use DQN](#header-49)
- [ã€RL basicsã€‘DQN's overestimation problem](#header-50)
- [ã€RL basicsã€‘Value-Based vs Policy-Based](#header-51)
- [ã€RL basicsã€‘Policy gradient](#header-52)
- [ã€RL basicsã€‘Multi-agent reinforcement learning(MARL)](#header-53)
- [ã€RL basicsã€‘Multi-agent DDPG <sup>[<a href="./references.md">41</a>]</sup>](#header-54)
- [ã€RL basicsã€‘Imitation learning(IL)](#header-55)
- [ã€RL basicsã€‘Behavior cloning(BC)](#header-56)
- [ã€RL basicsã€‘Inverse RL(IRL) and RL](#header-57)
- [ã€RL basicsã€‘Model-Based and Model-Free](#header-58)
- [ã€RL basicsã€‘Feudal RL](#header-59)
- [ã€RL basicsã€‘Distributional RL](#header-60)
- [ã€Policy Optimization & Variantsã€‘Actor-Critic](#header-61)
- [ã€Policy Optimization & Variantsã€‘Comparison of baseline and advantage](#header-62)
- [ã€Policy Optimization & Variantsã€‘GAE(Generalized Advantage Estimation)](#header-63)
- [ã€Policy Optimization & Variantsã€‘TRPO and its trust region](#header-64)
- [ã€Policy Optimization & Variantsã€‘Importance sampling](#header-65)
- [ã€Policy Optimization & Variantsã€‘PPO-Clip](#header-66)
- [ã€Policy Optimization & Variantsã€‘Policy model update process in PPO training](#header-67)
- [ã€Policy Optimization & Variantsã€‘PPO Pseudocode](#header-67-2)
- [ã€Policy Optimization & Variantsã€‘GRPO & PPO <sup>[<a href="./references.md">72</a>]</sup>](#header-68)
- [ã€Policy Optimization & Variantsã€‘Deterministic policy vs. Stochastic policy](#header-69)
- [ã€Policy Optimization & Variantsã€‘DPG](#header-70)
- [ã€Policy Optimization & Variantsã€‘DDPGï¼ˆDeep Deterministic Policy Gradientï¼‰](#header-71)
- [ã€RLHF and RLAIFã€‘RL modeling of language models](#header-72)
- [ã€RLHF and RLAIFã€‘Two-stage training process of RLHF](#header-73)
- [ã€RLHF and RLAIFã€‘Structure of the reward model](#header-74)
- [ã€RLHF and RLAIFã€‘Input and output of the reward model](#header-75)
- [ã€RLHF and RLAIFã€‘Reward deviation and loss](#header-76)
- [ã€RLHF and RLAIFã€‘Training of the reward model](#header-77)
- [ã€RLHF and RLAIFã€‘Relationship between the four models in PPO](#header-78)
- [ã€RLHF and RLAIFã€‘The structure and init of the four models in PPO](#header-79)
- [ã€RLHF and RLAIFã€‘A value model with a dual-head structure](#header-80)
- [ã€RLHF and RLAIFã€‘Four models can share one base in RLHF](#header-81)
- [ã€RLHF and RLAIFã€‘Inputs and Outputs of Each Model in PPO](#header-82)
- [ã€RLHF and RLAIFã€‘The Process of Calculating KL in PPO](#header-83)
- [ã€RLHF and RLAIFã€‘RLHF Training Based on PPO](#header-84)
- [ã€RLHF and RLAIFã€‘Rejection Sampling Fine-tuning](#header-85)
- [ã€RLHF and RLAIFã€‘RLAIF vs RLHF](#header-86)
- [ã€RLHF and RLAIFã€‘CAI(Constitutional AI)](#header-87)
- [ã€RLHF and RLAIFã€‘OpenAI RBR(Rule-Based Reward)](#header-88)
- [ã€Reasoning capacity optimizationã€‘Knowledge Distillation Based on CoT](#header-89)
- [ã€Reasoning capacity optimizationã€‘Distillation Based on DeepSeek](#header-90)
- [ã€Reasoning capacity optimizationã€‘ORM(Outcome Reward Model)Â &Â PRM (Process Reward Model)](#header-91)
- [ã€Reasoning capacity optimizationã€‘Four Key Steps of Each MCTS](#header-92)
- [ã€Reasoning capacity optimizationã€‘MCTS](#header-93)
- [ã€Reasoning capacity optimizationã€‘Search Tree Example in a Linguistic Context](#header-94)
- [ã€Reasoning capacity optimizationã€‘BoN(Best-of-N) Sampling](#header-95)
- [ã€Reasoning capacity optimizationã€‘Majority Vote](#header-96)
- [ã€Reasoning capacity optimizationã€‘Performance Growth of AlphaGo Zero <sup>[<a href="./references.md">179</a>]</sup>](#header-97)
- [ã€LLM basics extendedã€‘Performance Optimization Map for Large Models](#header-98)
- [ã€LLM basics extendedã€‘ALiBi positional encoding](#header-99)
- [ã€LLM basics extendedã€‘Traditional knowledge distillation](#header-100)
- [ã€LLM basics extendedã€‘Numerical representation, quantization](#header-101)
- [ã€LLM basics extendedã€‘Forward and backward](#header-102)
- [ã€LLM basics extendedã€‘Gradient Accumulation](#header-103)
- [ã€LLM basics extendedã€‘Gradient Checkpoint(gradient recomputation)](#header-104)
- [ã€LLM basics extendedã€‘Full recomputation ](#header-105)
- [ã€LLM basics extendedã€‘LLM Benchmark](#header-106)
- [ã€LLM basics extendedã€‘MHAã€GQAã€MQAã€MLA](#header-107)
- [ã€LLM basics extendedã€‘RNN(Recurrent Neural Network)](#header-108)
- [ã€LLM basics extendedã€‘Pre-norm vs Post-norm](#header-109)
- [ã€LLM basics extendedã€‘BatchNormÂ &Â LayerNorm](#header-110)
- [ã€LLM basics extendedã€‘RMSNorm](#header-111)
- [ã€LLM basics extendedã€‘Prune](#header-112)
- [ã€LLM basics extendedã€‘Role of the temperature coefficient](#header-113)
- [ã€LLM basics extendedã€‘SwiGLU](#header-114)
- [ã€LLM basics extendedã€‘AUCã€PRã€F1ã€Precisionã€Recall](#header-115)
- [ã€LLM basics extendedã€‘RoPE positional encoding](#header-116)
- [ã€LLM basics extendedã€‘The effect of RoPE on each sequence position and each dim](#header-117)
- [ğŸ“Œ For Reference Section](#header-118)
- [ğŸ“Œ BibTeX Citation Format](#header-119)


### <a name="header-1"></a>Overall Architecture of Large Model Algorithms (Focusing on LLMs and VLMs)

<img src="./assets/LLM-RL-Algorithms-en.png" alt="Overall Architecture of Large Model Algorithms">


### <a name="header-2"></a>ã€LLM basicsã€‘LLM overview
[![ã€LLM basicsã€‘LLM overview](../images_english/source_svg/%E3%80%90LLM%20basics%E3%80%91LLM%20overview.svg)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/source_svg/%E3%80%90LLM%20basics%E3%80%91LLM%20overview.svg)

### <a name="header-3"></a>ã€LLM basicsã€‘LLM structure
[![ã€LLM basicsã€‘LLM structure](../images_english/png_small/%E3%80%90LLM%20basics%E3%80%91LLM%20structure.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%E3%80%91LLM%20structure.png)

### <a name="header-4"></a>ã€LLM basicsã€‘LLM generation and decoding
[![ã€LLM basicsã€‘LLM generation and decoding](../images_english/png_small/%E3%80%90LLM%20basics%E3%80%91LLM%20generation%20and%20decoding.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%E3%80%91LLM%20generation%20and%20decoding.png)

### <a name="header-5"></a>ã€LLM basicsã€‘LLM Input
[![ã€LLM basicsã€‘LLM Input](../images_english/png_small/%E3%80%90LLM%20basics%E3%80%91LLM%20Input.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%E3%80%91LLM%20Input.png)

### <a name="header-6"></a>ã€LLM basicsã€‘LLM output
[![ã€LLM basicsã€‘LLM output](../images_english/png_small/%E3%80%90LLM%20basics%E3%80%91LLM%20output.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%E3%80%91LLM%20output.png)

### <a name="header-7"></a>ã€LLM basicsã€‘MLLM and VLM
[![ã€LLM basicsã€‘MLLM and VLM](../images_english/png_small/%E3%80%90LLM%20basics%E3%80%91MLLM%20and%20VLM.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%E3%80%91MLLM%20and%20VLM.png)

### <a name="header-8"></a>ã€LLM basicsã€‘LLM training process
[![ã€LLM basicsã€‘LLM training process](../images_english/png_small/%E3%80%90LLM%20basics%E3%80%91LLM%20training%20process.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%E3%80%91LLM%20training%20process.png)

### <a name="header-9"></a>ã€SFTã€‘Categories of fine-tuning techniques
[![ã€SFTã€‘Categories of fine-tuning techniques](../images_english/png_small/%E3%80%90SFT%E3%80%91Categories%20of%20fine-tuning%20techniques.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91Categories%20of%20fine-tuning%20techniques.png)

### <a name="header-10"></a>ã€SFTã€‘LoRA(1 of 2)
[![ã€SFTã€‘LoRA(1 of 2)](../images_english/png_small/%E3%80%90SFT%E3%80%91LoRA%281%20of%202%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91LoRA%281%20of%202%29.png)

### <a name="header-11"></a>ã€SFTã€‘LoRA(2 of 2)
[![ã€SFTã€‘LoRA(2 of 2)](../images_english/png_small/%E3%80%90SFT%E3%80%91LoRA%282%20of%202%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91LoRA%282%20of%202%29.png)

### <a name="header-12"></a>ã€SFTã€‘Prefix-Tuning
[![ã€SFTã€‘Prefix-Tuning](../images_english/png_small/%E3%80%90SFT%E3%80%91Prefix-Tuning.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91Prefix-Tuning.png)

### <a name="header-13"></a>ã€SFTã€‘Token ID and Token
[![ã€SFTã€‘Token ID and Token](../images_english/png_small/%E3%80%90SFT%E3%80%91Token%20ID%20and%20Token.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91Token%20ID%20and%20Token.png)

### <a name="header-14"></a>ã€SFTã€‘Loss of SFT(cross-entropy)
[![ã€SFTã€‘Loss of SFT(cross-entropy)](../images_english/png_small/%E3%80%90SFT%E3%80%91Loss%20of%20SFT%28cross-entropy%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91Loss%20of%20SFT%28cross-entropy%29.png)

### <a name="header-15"></a>ã€SFTã€‘Packing of multiple pieces of sample
[![ã€SFTã€‘Packing of multiple pieces of sample](../images_english/png_small/%E3%80%90SFT%E3%80%91Packing%20of%20multiple%20pieces%20of%20sample.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91Packing%20of%20multiple%20pieces%20of%20sample.png)

### <a name="header-16"></a>ã€DPOã€‘RLHF vs DPO
[![ã€DPOã€‘RLHF vs DPO](../images_english/png_small/%E3%80%90DPO%E3%80%91RLHF%20vs%20DPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90DPO%E3%80%91RLHF%20vs%20DPO.png)

### <a name="header-17"></a>ã€DPOã€‘DPO(Direct Preference Optimization)
[![ã€DPOã€‘DPO(DirectPreferenceOptimization)](../images_english/png_small/%E3%80%90DPO%E3%80%91DPO%28DirectPreferenceOptimization%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90DPO%E3%80%91DPO%28DirectPreferenceOptimization%29.png)

### <a name="header-18"></a>ã€DPOã€‘Overview of DPO training
[![ã€DPOã€‘Overview of DPO training](../images_english/png_small/%E3%80%90DPO%E3%80%91Overview%20of%20DPO%20training.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90DPO%E3%80%91Overview%20of%20DPO%20training.png)

### <a name="header-19"></a>ã€DPOã€‘Impact of the Î² parameter on DPO
[![ã€DPOã€‘Impact of the Î² parameter on DPO](../images_english/png_small/%E3%80%90DPO%E3%80%91Impact%20of%20the%20%CE%B2%20parameter%20on%20DPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90DPO%E3%80%91Impact%20of%20the%20%CE%B2%20parameter%20on%20DPO.png)

### <a name="header-20"></a>ã€DPOã€‘Effect of implicit reward differences on the magnitude of parameter updates
[![ã€DPOã€‘Effect of implicit reward differences on the magnitude of parameter updates](../images_english/png_small/%E3%80%90DPO%E3%80%91Effect%20of%20implicit%20reward%20differences%20on%20the%20magnitude%20of%20parameter%20updates.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90DPO%E3%80%91Effect%20of%20implicit%20reward%20differences%20on%20the%20magnitude%20of%20parameter%20updates.png)

### <a name="header-21"></a>ã€Optimization without trainingã€‘Comparison of CoT and traditional Q&A
[![ã€Optimization without trainingã€‘Comparison of CoT and traditional Q&A](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Comparison%20of%20CoT%20and%20traditional%20Q%26A.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Comparison%20of%20CoT%20and%20traditional%20Q%26A.png)

### <a name="header-22"></a>ã€Optimization without trainingã€‘CoTã€Self-consistency CoTã€ToTã€GoT <sup>[<a href="./references.md">87</a>]</sup>
[![ã€Optimization without trainingã€‘CoTã€Self-consistencyCoTã€ToTã€GoT](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91CoT%E3%80%81Self-consistencyCoT%E3%80%81ToT%E3%80%81GoT.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91CoT%E3%80%81Self-consistencyCoT%E3%80%81ToT%E3%80%81GoT.png)

### <a name="header-23"></a>ã€Optimization without trainingã€‘Exhaustive Search
[![ã€Optimization without trainingã€‘Exhaustive Search](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Exhaustive%20Search.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Exhaustive%20Search.png)

### <a name="header-24"></a>ã€Optimization without trainingã€‘Greedy Search
[![ã€Optimization without trainingã€‘Greedy Search](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Greedy%20Search.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Greedy%20Search.png)

### <a name="header-25"></a>ã€Optimization without trainingã€‘Beam Search
[![ã€Optimization without trainingã€‘Beam Search](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Beam%20Search.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Beam%20Search.png)

### <a name="header-26"></a>ã€Optimization without trainingã€‘Multinomial Sampling
[![ã€Optimization without trainingã€‘Multinomial Sampling](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Multinomial%20Sampling.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Multinomial%20Sampling.png)

### <a name="header-27"></a>ã€Optimization without trainingã€‘Top-K Sampling
[![ã€Optimization without trainingã€‘Top-K Sampling](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Top-K%20Sampling.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Top-K%20Sampling.png)

### <a name="header-28"></a>ã€Optimization without trainingã€‘Top-P Sampling
[![ã€Optimization without trainingã€‘Top-P Sampling](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Top-P%20Sampling.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Top-P%20Sampling.png)

### <a name="header-29"></a>ã€Optimization without trainingã€‘RAG(Retrieval-Augmented Generation)
[![ã€Optimization without trainingã€‘RAG(Retrieval-Augmented Generation)](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91RAG%28Retrieval-Augmented%20Generation%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91RAG%28Retrieval-Augmented%20Generation%29.png)

### <a name="header-30"></a>ã€Optimization without trainingã€‘Function Calling
[![ã€Optimization without trainingã€‘Function Calling](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Function%20Calling.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Function%20Calling.png)

### <a name="header-31"></a>ã€RL basicsã€‘History of RL
[![ã€RL basicsã€‘History of RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91History%20of%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91History%20of%20RL.png)

### <a name="header-32"></a>ã€RL basicsã€‘Three major machine learning paradigms
[![ã€RL basicsã€‘Three major machine learning paradigms](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Three%20major%20machine%20learning%20paradigms.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Three%20major%20machine%20learning%20paradigms.png)

### <a name="header-33"></a>ã€RL basicsã€‘Basic architecture of RL
[![ã€RL basicsã€‘Basic architecture of RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Basic%20architecture%20of%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Basic%20architecture%20of%20RL.png)

### <a name="header-34"></a>ã€RL basicsã€‘Fundamental Concepts of RL
[![ã€RL basicsã€‘Fundamental Concepts of RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Fundamental%20Concepts%20of%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Fundamental%20Concepts%20of%20RL.png)

### <a name="header-35"></a>ã€RL basicsã€‘Markov Chain vs MDP
[![ã€RL basicsã€‘Markov Chain vs MDP](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Markov%20Chain%20vs%20MDP.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Markov%20Chain%20vs%20MDP.png)

### <a name="header-36"></a>ã€RL basicsã€‘Using dynamic Îµ values under the Îµ-greedy strategy
[![ã€RL basicsã€‘Using dynamic Îµ values under the Îµ-greedy strategy](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Using%20dynamic%20%CE%B5%20values%20under%20the%20%CE%B5-greedy%20strategy.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Using%20dynamic%20%CE%B5%20values%20under%20the%20%CE%B5-greedy%20strategy.png)

### <a name="header-37"></a>ã€RL basicsã€‘Comparison of RL training paradigms
- On-policyï¼ŒOff-policyï¼ŒOffline RL
[![ã€RL basicsã€‘Comparison of RL training paradigms](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Comparison%20of%20RL%20training%20paradigms.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Comparison%20of%20RL%20training%20paradigms.png)

### <a name="header-38"></a>ã€RL basicsã€‘Classification of RL
[![ã€RL basicsã€‘Classification of RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Classification%20of%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Classification%20of%20RL.png)

### <a name="header-39"></a>ã€RL basicsã€‘Return(cumulative reward)
[![ã€RL basicsã€‘Return(cumulative reward)](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Return%28cumulative%20reward%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Return%28cumulative%20reward%29.png)

### <a name="header-40"></a>ã€RL basicsã€‘Backwards iteration and computation of return G
[![ã€RL basicsã€‘Backwards iteration and computation of return G](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Backwards%20iteration%20and%20computation%20of%20return%20G.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Backwards%20iteration%20and%20computation%20of%20return%20G.png)

### <a name="header-41"></a>ã€RL basicsã€‘Reward, Return, and Value
[![ã€RL basicsã€‘Reward, Return, and Value](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Reward%2C%20Return%2C%20and%20Value.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Reward%2C%20Return%2C%20and%20Value.png)

### <a name="header-42"></a>ã€RL basicsã€‘QÏ€ and VÏ€
[![ã€RL basicsã€‘QÏ€ and VÏ€](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Q%CF%80%20and%20V%CF%80.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Q%CF%80%20and%20V%CF%80.png)

### <a name="header-43"></a>ã€RL basicsã€‘Estimate the value through Monte Carlo(MC)
[![ã€RL basicsã€‘Estimate the value through Monte Carlo(MC)](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Estimate%20the%20value%20through%20Monte%20Carlo%28MC%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Estimate%20the%20value%20through%20Monte%20Carlo%28MC%29.png)

### <a name="header-44"></a>ã€RL basicsã€‘TD target and TD error
[![ã€RL basicsã€‘TD target and TD error](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91TD%20target%20and%20TD%20error.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91TD%20target%20and%20TD%20error.png)

### <a name="header-45"></a>ã€RL basicsã€‘TD(0), n-step TD, and MC
[![ã€RL basicsã€‘TD(0), n-step TD, and MC](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91TD%280%29%2C%20n-step%20TD%2C%20and%20MC.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91TD%280%29%2C%20n-step%20TD%2C%20and%20MC.png)

### <a name="header-46"></a>ã€RL basicsã€‘Characteristics of MC and TD methods
[![ã€RL basicsã€‘Characteristics of MC and TD methods](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Characteristics%20of%20MC%20and%20TD%20methods.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Characteristics%20of%20MC%20and%20TD%20methods.png)

### <a name="header-47"></a>ã€RL basicsã€‘MC, TD, DP, and exhaustive search <sup>[<a href="./references.md">32</a>]</sup>
[![ã€RL basicsã€‘MC, TD, DP, and exhaustive search](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91MC%2C%20TD%2C%20DP%2C%20and%20exhaustive%20search.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91MC%2C%20TD%2C%20DP%2C%20and%20exhaustive%20search.png)

### <a name="header-48"></a>ã€RL basicsã€‘DQN model with two input-output structures
[![ã€RL basicsã€‘DQN model with two input-output structures](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91DQN%20model%20with%20two%20input-output%20structures.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91DQN%20model%20with%20two%20input-output%20structures.png)

### <a name="header-49"></a>ã€RL basicsã€‘How to use DQN
[![ã€RL basicsã€‘How to use DQN](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91How%20to%20use%20DQN.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91How%20to%20use%20DQN.png)

### <a name="header-50"></a>ã€RL basicsã€‘DQN's overestimation problem
[![ã€RL basicsã€‘DQN's overestimation problem](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91DQN%27s%20overestimation%20problem.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91DQN%27s%20overestimation%20problem.png)

### <a name="header-51"></a>ã€RL basicsã€‘Value-Based vs Policy-Based
[![ã€RL basicsã€‘Value-Based vs Policy-Based](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Value-Based%20vs%20Policy-Based.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Value-Based%20vs%20Policy-Based.png)

### <a name="header-52"></a>ã€RL basicsã€‘Policy gradient
[![ã€RL basicsã€‘Policy gradient](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Policy%20gradient.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Policy%20gradient.png)

### <a name="header-53"></a>ã€RL basicsã€‘Multi-agent reinforcement learning(MARL)
[![ã€RL basicsã€‘Multi-agent reinforcement learning(MARL)](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Multi-agent%20reinforcement%20learning%28MARL%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Multi-agent%20reinforcement%20learning%28MARL%29.png)

### <a name="header-54"></a>ã€RL basicsã€‘Multi-agent DDPG <sup>[<a href="./references.md">41</a>]</sup>
[![ã€RL basicsã€‘Multi-agent DDPG](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Multi-agent%20DDPG.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Multi-agent%20DDPG.png)

### <a name="header-55"></a>ã€RL basicsã€‘Imitation learning(IL)
[![ã€RL basicsã€‘Imitation learning(IL)](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Imitation%20learning%28IL%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Imitation%20learning%28IL%29.png)

### <a name="header-56"></a>ã€RL basicsã€‘Behavior cloning(BC)
[![ã€RL basicsã€‘Behavior cloning(BC)](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Behavior%20cloning%28BC%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Behavior%20cloning%28BC%29.png)

### <a name="header-57"></a>ã€RL basicsã€‘Inverse RL(IRL) and RL
[![ã€RL basicsã€‘Inverse RL(IRL) and RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Inverse%20RL%28IRL%29%20and%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Inverse%20RL%28IRL%29%20and%20RL.png)

### <a name="header-58"></a>ã€RL basicsã€‘Model-Based and Model-Free
[![ã€RL basicsã€‘Model-Based and Model-Free](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Model-Based%20and%20Model-Free.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Model-Based%20and%20Model-Free.png)

### <a name="header-59"></a>ã€RL basicsã€‘Feudal RL
[![ã€RL basicsã€‘Feudal RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Feudal%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Feudal%20RL.png)

### <a name="header-60"></a>ã€RL basicsã€‘Distributional RL
[![ã€RL basicsã€‘Distributional RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Distributional%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Distributional%20RL.png)

### <a name="header-61"></a>ã€Policy Optimization & Variantsã€‘Actor-Critic
[![ã€Policy Optimization & Variantsã€‘Actor-Critic](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Actor-Critic.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Actor-Critic.png)

### <a name="header-62"></a>ã€Policy Optimization & Variantsã€‘Comparison of baseline and advantage
[![ã€Policy Optimization & Variantsã€‘Comparison of baseline and advantage](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Comparison%20of%20baseline%20and%20advantage.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Comparison%20of%20baseline%20and%20advantage.png)

### <a name="header-63"></a>ã€Policy Optimization & Variantsã€‘GAE(Generalized Advantage Estimation)
[![ã€Policy Optimization & Variantsã€‘GAE](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91GAE.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91GAE.png)

### <a name="header-64"></a>ã€Policy Optimization & Variantsã€‘TRPO and its trust region
[![ã€Policy Optimization & Variantsã€‘TRPO and its trust region](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91TRPO%20and%20its%20trust%20region.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91TRPO%20and%20its%20trust%20region.png)

### <a name="header-65"></a>ã€Policy Optimization & Variantsã€‘Importance sampling
[![ã€Policy Optimization & Variantsã€‘Importance sampling](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Importance%20sampling.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Importance%20sampling.png)

### <a name="header-66"></a>ã€Policy Optimization & Variantsã€‘PPO-Clip
[![ã€Policy Optimization & Variantsã€‘PPO-Clip](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91PPO-Clip.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91PPO-Clip.png)

### <a name="header-67"></a>ã€Policy Optimization & Variantsã€‘Policy model update process in PPO training
[![ã€Policy Optimization & Variantsã€‘Policy model update process in PPO training](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Policy%20model%20update%20process%20in%20PPO%20training.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Policy%20model%20update%20process%20in%20PPO%20training.png)

### <a name="header-67-2"></a>ã€Policy Optimization & Variantsã€‘PPO Pseudocode
[![ã€Policy Optimization & Variantsã€‘Policy model update process in PPO training](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Policy%20model%20update%20process%20in%20PPO%20training.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Policy%20model%20update%20process%20in%20PPO%20training.png)

```python
# Abbreviations: R = rewards, V = values, Adv = advantages, J = objective, P = probability
for iteration in range(num_iterations):  # Perform num_iterations training iterations
    # [1/2] Collect samples (prompt, response_old, logP_old, Adv, V_target)
    prompt_batch, response_old_batch = [], []
    logP_old_batch, Adv_batch, V_target_batch = [], [], []
    for _ in range(num_examples):
        logP_old, response_old  = actor_model(prompt)
        V_old    = critic_model(prompt, response_old)
        R        = reward_model(prompt, response_old)[-1]
        logP_ref = ref_model(prompt, response_old)
        
        # KL penalty. Note: R here is only the reward for the final token
        KL = logP_old - logP_ref
        R_with_KL = R - scale_factor * KL

        # Compute advantage Adv via GAE
        Adv = GAE_Advantage(R_with_KL, V_old, gamma, Î»)
        V_target = Adv + V_old

        prompt_batch        += prompt
        response_old_batch  += response_old
        logP_old_batch      += logP_old
        Adv_batch           += Adv
        V_target_batch      += V_target

    # [2/2] PPO training loop: multiple parameter updates
    for _ in range(ppo_epochs):
        mini_batches = shuffle_split(
            (prompt_batch, response_old_batch, logP_old_batch, Adv_batch, V_target_batch),
            mini_batch_size
        )
        
        for prompt, response_old, logP_old, Adv, V_target in mini_batches:
            logits, logP_new = actor_model(prompt, response_old)
            V_new            = critic_model(prompt, response_old)

            # Probability ratio: ratio(Î¸) = Ï€_Î¸(a|s) / Ï€_{Î¸_old}(a|s)
            ratios = exp(logP_new - logP_old)

            # Compute clipped policy loss
            L_clip = -mean(
                min(ratios * Adv,
                    clip(ratios, 1 - Îµ, 1 + Îµ) * Adv)
            )
            
            S_entropy = mean(compute_entropy(logits))  # Compute policy entropy

            Loss_V = mean((V_new - V_target) ** 2)     # Compute value function loss

            # Total loss
            Loss = L_clip + C1 * Loss_V - C2 * S_entropy

            backward_update(Loss, L_clip, Loss_V)      # Backpropagate and update parameters
```

### <a name="header-68"></a>ã€Policy Optimization & Variantsã€‘GRPO & PPO <sup>[<a href="./references.md">72</a>]</sup>
[![ã€Policy Optimization & Variantsã€‘GRPO & PPO](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91GRPO%20%26%20PPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91GRPO%20%26%20PPO.png)

### <a name="header-69"></a>ã€Policy Optimization & Variantsã€‘Deterministic policy vs. Stochastic policy
[![ã€Policy Optimization & Variantsã€‘Deterministic policy vs. Stochastic policy](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Deterministic%20policy%20vs.%20Stochastic%20policy.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Deterministic%20policy%20vs.%20Stochastic%20policy.png)

### <a name="header-70"></a>ã€Policy Optimization & Variantsã€‘DPG
[![ã€Policy Optimization & Variantsã€‘DPG](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91DPG.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91DPG.png)

### <a name="header-71"></a>ã€Policy Optimization & Variantsã€‘DDPGï¼ˆDeep Deterministic Policy Gradientï¼‰
[![ã€Policy Optimization & Variantsã€‘DDPG](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91DDPG.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91DDPG.png)

### <a name="header-72"></a>ã€RLHF and RLAIFã€‘RL modeling of language models
[![ã€RLHF and RLAIFã€‘RL modeling of language models](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91RL%20modeling%20of%20language%20models.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91RL%20modeling%20of%20language%20models.png)

### <a name="header-73"></a>ã€RLHF and RLAIFã€‘Two-stage training process of RLHF
[![ã€RLHF and RLAIFã€‘Two-stage training process of RLHF](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Two-stage%20training%20process%20of%20RLHF.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Two-stage%20training%20process%20of%20RLHF.png)

### <a name="header-74"></a>ã€RLHF and RLAIFã€‘Structure of the reward model
[![ã€RLHF and RLAIFã€‘Structure of the reward model](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Structure%20of%20the%20reward%20model.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Structure%20of%20the%20reward%20model.png)

### <a name="header-75"></a>ã€RLHF and RLAIFã€‘Input and output of the reward model
[![ã€RLHF and RLAIFã€‘Input and output of the reward model](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Input%20and%20output%20of%20the%20reward%20model.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Input%20and%20output%20of%20the%20reward%20model.png)

### <a name="header-76"></a>ã€RLHF and RLAIFã€‘Reward deviation and loss
[![ã€RLHF and RLAIFã€‘Reward deviation and loss](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Reward%20deviation%20and%20loss.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Reward%20deviation%20and%20loss.png)

### <a name="header-77"></a>ã€RLHF and RLAIFã€‘Training of the reward model
[![ã€RLHF and RLAIFã€‘Training of the reward model](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Training%20of%20the%20reward%20model.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Training%20of%20the%20reward%20model.png)

### <a name="header-78"></a>ã€RLHF and RLAIFã€‘Relationship between the four models in PPO
[![ã€RLHF and RLAIFã€‘Relationship between the four models in PPO](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Relationship%20between%20the%20four%20models%20in%20PPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Relationship%20between%20the%20four%20models%20in%20PPO.png)

### <a name="header-79"></a>ã€RLHF and RLAIFã€‘The structure and init of the four models in PPO
[![ã€RLHF and RLAIFã€‘The structure and init of the four models in PPO](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91The%20structure%20and%20init%20of%20the%20four%20models%20in%20PPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91The%20structure%20and%20init%20of%20the%20four%20models%20in%20PPO.png)

### <a name="header-80"></a>ã€RLHF and RLAIFã€‘A value model with a dual-head structure
[![ã€RLHF and RLAIFã€‘A value model with a dual-head structure](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91A%20value%20model%20with%20a%20dual-head%20structure.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91A%20value%20model%20with%20a%20dual-head%20structure.png)

### <a name="header-81"></a>ã€RLHF and RLAIFã€‘Four models can share one base in RLHF
[![ã€RLHF and RLAIFã€‘Four models can share one base in RLHF](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Four%20models%20can%20share%20one%20base%20in%20RLHF.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Four%20models%20can%20share%20one%20base%20in%20RLHF.png)

### <a name="header-82"></a>ã€RLHF and RLAIFã€‘Inputs and Outputs of Each Model in PPO
[![ã€RLHF and RLAIFã€‘Inputs and Outputs of Each Model in PPO](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Inputs%20and%20Outputs%20of%20Each%20Model%20in%20PPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Inputs%20and%20Outputs%20of%20Each%20Model%20in%20PPO.png)

### <a name="header-83"></a>ã€RLHF and RLAIFã€‘The Process of Calculating KL in PPO
[![ã€RLHF and RLAIFã€‘The Process of Calculating KL in PPO](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91The%20Process%20of%20Calculating%20KL%20in%20PPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91The%20Process%20of%20Calculating%20KL%20in%20PPO.png)

### <a name="header-84"></a>ã€RLHF and RLAIFã€‘RLHF Training Based on PPO
[![ã€RLHF and RLAIFã€‘RLHF Training Based on PPO](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91RLHF%20Training%20Based%20on%20PPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91RLHF%20Training%20Based%20on%20PPO.png)

### <a name="header-85"></a>ã€RLHF and RLAIFã€‘Rejection Sampling Fine-tuning
[![ã€RLHF and RLAIFã€‘Rejection Sampling Fine-tuning](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Rejection%20Sampling%20Fine-tuning.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Rejection%20Sampling%20Fine-tuning.png)

### <a name="header-86"></a>ã€RLHF and RLAIFã€‘RLAIF vs RLHF
[![ã€RLHF and RLAIFã€‘RLAIF vs RLHF](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91RLAIF%20vs%20RLHF.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91RLAIF%20vs%20RLHF.png)

### <a name="header-87"></a>ã€RLHF and RLAIFã€‘CAI(Constitutional AI)
[![ã€RLHF and RLAIFã€‘CAI(Constitutional AI)](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91CAI%28Constitutional%20AI%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91CAI%28Constitutional%20AI%29.png)

### <a name="header-88"></a>ã€RLHF and RLAIFã€‘OpenAI RBR(Rule-Based Reward)
[![ã€RLHF and RLAIFã€‘OpenAI RBR(Rule-Based Reward)](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91OpenAI%20RBR%28Rule-Based%20Reward%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91OpenAI%20RBR%28Rule-Based%20Reward%29.png)

### <a name="header-89"></a>ã€Reasoning capacity optimizationã€‘Knowledge Distillation Based on CoT
[![ã€Reasoning capacity optimizationã€‘Knowledge Distillation Based on CoT](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Knowledge%20Distillation%20Based%20on%20CoT.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Knowledge%20Distillation%20Based%20on%20CoT.png)

### <a name="header-90"></a>ã€Reasoning capacity optimizationã€‘Distillation Based on DeepSeek
[![ã€Reasoning capacity optimizationã€‘Distillation Based on DeepSeek](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Distillation%20Based%20on%20DeepSeek.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Distillation%20Based%20on%20DeepSeek.png)

### <a name="header-91"></a>ã€Reasoning capacity optimizationã€‘ORM(Outcome Reward Model)Â &Â PRM (Process Reward Model)
[![ã€Reasoning capacity optimizationã€‘ORMÂ &Â PRM](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91ORM%C2%A0%26%C2%A0PRM.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91ORM%C2%A0%26%C2%A0PRM.png)

### <a name="header-92"></a>ã€Reasoning capacity optimizationã€‘Four Key Steps of Each MCTS
[![ã€Reasoning capacity optimizationã€‘Four Key Steps of Each MCTS](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Four%20Key%20Steps%20of%20Each%20MCTS.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Four%20Key%20Steps%20of%20Each%20MCTS.png)

### <a name="header-93"></a>ã€Reasoning capacity optimizationã€‘MCTS
[![ã€Reasoning capacity optimizationã€‘MCTS](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91MCTS.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91MCTS.png)

### <a name="header-94"></a>ã€Reasoning capacity optimizationã€‘Search Tree Example in a Linguistic Context
[![ã€Reasoning capacity optimizationã€‘Search Tree Example in a Linguistic Context](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Search%20Tree%20Example%20in%20a%20Linguistic%20Context.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Search%20Tree%20Example%20in%20a%20Linguistic%20Context.png)

### <a name="header-95"></a>ã€Reasoning capacity optimizationã€‘BoN(Best-of-N) Sampling
[![ã€Reasoning capacity optimizationã€‘BoN(Best-of-N) Sampling](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91BoN%28Best-of-N%29%20Sampling.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91BoN%28Best-of-N%29%20Sampling.png)

### <a name="header-96"></a>ã€Reasoning capacity optimizationã€‘Majority Vote
[![ã€Reasoning capacity optimizationã€‘Majority Vote](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Majority%20Vote.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Majority%20Vote.png)

### <a name="header-97"></a>ã€Reasoning capacity optimizationã€‘Performance Growth of AlphaGo Zero <sup>[<a href="./references.md">179</a>]</sup>
[![ã€Reasoning capacity optimizationã€‘Performance Growth of AlphaGo Zero](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Performance%20Growth%20of%20AlphaGo%20Zero.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Performance%20Growth%20of%20AlphaGo%20Zero.png)

### <a name="header-98"></a>ã€LLM basics extendedã€‘Performance Optimization Map for Large Models
[![ã€LLM basics extendedã€‘Performance Optimization Map for Large Models](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Performance%20Optimization%20Map%20for%20Large%20Models.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Performance%20Optimization%20Map%20for%20Large%20Models.png)

### <a name="header-99"></a>ã€LLM basics extendedã€‘ALiBi positional encoding
[![ã€LLM basics extendedã€‘ALiBi positional encoding](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91ALiBi%20positional%20encoding.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91ALiBi%20positional%20encoding.png)

### <a name="header-100"></a>ã€LLM basics extendedã€‘Traditional knowledge distillation
[![ã€LLM basics extendedã€‘Traditional knowledge distillation](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Traditional%20knowledge%20distillation.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Traditional%20knowledge%20distillation.png)

### <a name="header-101"></a>ã€LLM basics extendedã€‘Numerical representation, quantization
[![ã€LLM basics extendedã€‘Numerical representation, quantization](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Numerical%20representation%2C%20quantization.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Numerical%20representation%2C%20quantization.png)

### <a name="header-102"></a>ã€LLM basics extendedã€‘Forward and backward
[![ã€LLM basics extendedã€‘Forward and backward](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Forward%20and%20backward.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Forward%20and%20backward.png)

### <a name="header-103"></a>ã€LLM basics extendedã€‘Gradient Accumulation
[![ã€LLM basics extendedã€‘Gradient Accumulation](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Gradient%20Accumulation.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Gradient%20Accumulation.png)

### <a name="header-104"></a>ã€LLM basics extendedã€‘Gradient Checkpoint(gradient recomputation)
[![ã€LLM basics extendedã€‘Gradient Checkpoint(gradient recomputation)](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Gradient%20Checkpoint%28gradient%20recomputation%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Gradient%20Checkpoint%28gradient%20recomputation%29.png)

### <a name="header-105"></a>ã€LLM basics extendedã€‘Full recomputation 
[![ã€LLM basics extendedã€‘Full recomputation ](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Full%20recomputation%20.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Full%20recomputation%20.png)

### <a name="header-106"></a>ã€LLM basics extendedã€‘LLM Benchmark
[![ã€LLM basics extendedã€‘LLM Benchmark](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91LLM%20Benchmark.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91LLM%20Benchmark.png)

### <a name="header-107"></a>ã€LLM basics extendedã€‘MHAã€GQAã€MQAã€MLA
[![ã€LLM basics extendedã€‘MHAã€GQAã€MQAã€MLA](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91MHA%E3%80%81GQA%E3%80%81MQA%E3%80%81MLA.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91MHA%E3%80%81GQA%E3%80%81MQA%E3%80%81MLA.png)

### <a name="header-108"></a>ã€LLM basics extendedã€‘RNN(Recurrent Neural Network)
[![ã€LLM basics extendedã€‘RNN](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91RNN.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91RNN.png)

### <a name="header-109"></a>ã€LLM basics extendedã€‘Pre-norm vs Post-norm
[![ã€LLM basics extendedã€‘Pre-norm vs Post-norm](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Pre-norm%20vs%20Post-norm.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Pre-norm%20vs%20Post-norm.png)

### <a name="header-110"></a>ã€LLM basics extendedã€‘BatchNormÂ &Â LayerNorm
[![ã€LLM basics extendedã€‘BatchNormÂ &Â LayerNorm](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91BatchNorm%C2%A0%26%C2%A0LayerNorm.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91BatchNorm%C2%A0%26%C2%A0LayerNorm.png)

### <a name="header-111"></a>ã€LLM basics extendedã€‘RMSNorm
[![ã€LLM basics extendedã€‘RMSNorm](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91RMSNorm.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91RMSNorm.png)

### <a name="header-112"></a>ã€LLM basics extendedã€‘Prune
[![ã€LLM basics extendedã€‘Prune](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Prune.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Prune.png)

### <a name="header-113"></a>ã€LLM basics extendedã€‘Role of the temperature coefficient
[![ã€LLM basics extendedã€‘Role of the temperature coefficient](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Role%20of%20the%20temperature%20coefficient.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Role%20of%20the%20temperature%20coefficient.png)

### <a name="header-114"></a>ã€LLM basics extendedã€‘SwiGLU
[![ã€LLM basics extendedã€‘SwiGLU](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91SwiGLU.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91SwiGLU.png)

### <a name="header-115"></a>ã€LLM basics extendedã€‘AUCã€PRã€F1ã€Precisionã€Recall
[![ã€LLM basics extendedã€‘AUCã€PRã€F1ã€Precisionã€Recall](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91AUC%E3%80%81PR%E3%80%81F1%E3%80%81Precision%E3%80%81Recall.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91AUC%E3%80%81PR%E3%80%81F1%E3%80%81Precision%E3%80%81Recall.png)

### <a name="header-116"></a>ã€LLM basics extendedã€‘RoPE positional encoding
[![ã€LLM basics extendedã€‘RoPE positional encoding](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91RoPE%20positional%20encoding.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91RoPE%20positional%20encoding.png)

### <a name="header-117"></a>ã€LLM basics extendedã€‘The effect of RoPE on each sequence position and each dim
- For details on the principles of RoPE, the base and Î¸ values, and how they work, see: [RoPE-theta-base.xlsx](./RoPE-theta-base.xlsx) 


[![ã€LLM basics extendedã€‘The effect of RoPE on each sequence position and each dim](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91The%20effect%20of%20RoPE%20on%20each%20sequence%20position%20and%20each%20dim.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91The%20effect%20of%20RoPE%20on%20each%20sequence%20position%20and%20each%20dim.png)



<br>

---


## Contributing
- **Contributions are welcome!** Whether it's new diagrams, documentation, error corrections, or other improvements. You're welcome to include your name or nickname in the diagrams. Your GitHub account will also be listed in the **Contributors** to help more people discover you and your work. Example diagram template: [images-template.pptx](./assets/images-template.pptx)


-  **How to contribute:**  
  (1) Fork: Click the "Fork" button to create a copy of the repo under your GitHub account â†’  
  (2) Clone: Clone the forked repo to your local environment â†’  
  (3) Create a new local branch â†’  
  (4) Make changes and commit â†’  
  (5) Push changes to your remote repo â†’  
  (6) Submit a PR: On GitHub, go to your forked repo and click "Compare & pull request" to submit a PR. The maintainer will review and merge it into the main repository.

-  **Suggested color** scheme for diagram design:  
  <span style="display:inline-block;width:12px;height:12px;background-color:#71CCF5;border-radius:2px;margin-right:8px;"></span>Light Blue (`#71CCF5`) ;  
  <span style="display:inline-block;width:12px;height:12px;background-color:#FFE699;border-radius:2px;margin-right:8px;"></span>Light Yellow (`#FFE699`) ;  
  <span style="display:inline-block;width:12px;height:12px;background-color:#C0BFDE;border-radius:2px;margin-right:8px;"></span>Blue-Purple (`#C0BFDE`) ;  
  <span style="display:inline-block;width:12px;height:12px;background-color:#F0ADB7;border-radius:2px;margin-right:8px;"></span>Pink (`#F0ADB7`)


## Terms of Use
All images in this repository are licensed under [LICENSE](../LICENSE). You are free to use, modify, and remix the materials under the following terms:  
- **Sharing** â€” You may copy and redistribute the material in any format.  
- **Adapting** â€” You may remix, transform, and build upon the material.

You must also comply with the following terms:  
- **For Web Use** â€” If using the materials in blog posts or online content, please retain the original author information embedded in the images.  
- **For Papers, Books, and Publications** â€” If using the materials in formal publications, please cite the source using the format below. In such cases, the embedded author info may be removed from the image.  
- **Non-commercial Use Only** â€” These materials may not be used for any direct commercial purposes.



## Citation

If you use any content from this project (including diagrams or concepts from the book), please cite it as follows:

#### <a name="header-118"></a>ğŸ“Œ For Reference Section
```
Yu, Changye. Large Model Algorithms: Reinforcement Learning, Fine-Tuning, and Alignment. 
Beijing: Publishing House of Electronics Industry, 2025. https://github.com/changyeyu/LLM-RL-Visualized
```

#### <a name="header-119"></a>ğŸ“Œ BibTeX Citation Format
```bibtex
@book{yu2025largemodel_en,
  title     = {Large Model Algorithms: Reinforcement Learning, Fine-Tuning, and Alignment},
  author    = {Yu, Changye},
  publisher = {Publishing House of Electronics Industry},
  year      = {2025},
  address   = {Beijing},
  isbn      = {9787121500725},
  url       = {https://github.com/changyeyu/LLM-RL-Visualized},
  language  = {en}
}
```

---

<div align="center"> Continuously <strong> updating</strong>...   Click â­<strong>Star</strong> at the top-right to follow! </div> 