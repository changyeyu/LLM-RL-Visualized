<p align="center">
  <img src="../src/assets/banner/å¹»ç¯ç‰‡1.SVG" alt="Illustrated Large Model Algorithms: LLM, RL, VLM ..." />
</p>


<p align="center">
    <a href="./README_EN.md">
    <img
      alt="English Version"
      src="https://img.shields.io/badge/English-Version-blue?style=for-the-badge"
      width="250"
      height="50"
    />
  </a> 
  &nbsp; &nbsp;&nbsp;

  <a href="../README.md">
    <img
      alt="Chinese ä¸­æ–‡ç‰ˆæœ¬"
      src="https://img.shields.io/badge/Chinese-ä¸­æ–‡ç‰ˆæœ¬-red?style=for-the-badge"
      width="250"
      height="50"
    />
  </a>    

</p>




---
## Description

ğŸ‰ **100+ original diagrams**visualizing core concepts of large model algorithms â€” from LLMs, VLMs, and training methods (RL, RLHF, GRPO, DPO, SFT, distillation) to RAG and performance optimization.

ğŸ‰ Originally based on the diagrams from the Chinese **book** ğŸ“˜ã€Šå¤§æ¨¡å‹ç®—æ³•ï¼šå¼ºåŒ–å­¦ä¹ ã€å¾®è°ƒä¸å¯¹é½ã€‹ (<em>Large Model Algorithms: Reinforcement Learning, Fine-Tuning, and Alignment</em>), this project has since been continuously expanded with new content and improvements.

ğŸ‰ **Continuously updated** and actively maintained â€” click **Star â­** to stay tuned!

ğŸ‰ Contributions welcome â€” see the <a href="#contributing" style="color:rgb(44, 46, 49); text-decoration: underline;">Contributing Guide</a> to get involved.

Click on the images to view high-resolution versions, or browse the `.svg` vector files in the repository for infinite zoom support.

### Overall Architecture of Large Model Algorithms (Focusing on LLMs and VLMs)

<img src="./assets/LLM-RL-Algorithms-en.png" alt="Overall Architecture of Large Model Algorithms">


### ã€LLM basicsã€‘LLM overview
[![ã€LLM basicsã€‘LLM overview](../images_english/source_svg/%E3%80%90LLM%20basics%E3%80%91LLM%20overview.svg)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/source_svg/%E3%80%90LLM%20basics%E3%80%91LLM%20overview.svg)

### ã€LLM basicsã€‘LLM structure
[![ã€LLM basicsã€‘LLM structure](../images_english/png_small/%E3%80%90LLM%20basics%E3%80%91LLM%20structure.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%E3%80%91LLM%20structure.png)

### ã€LLM basicsã€‘LLM generation and decoding
[![ã€LLM basicsã€‘LLM generation and decoding](../images_english/png_small/%E3%80%90LLM%20basics%E3%80%91LLM%20generation%20and%20decoding.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%E3%80%91LLM%20generation%20and%20decoding.png)

### ã€LLM basicsã€‘LLM Input
[![ã€LLM basicsã€‘LLM Input](../images_english/png_small/%E3%80%90LLM%20basics%E3%80%91LLM%20Input.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%E3%80%91LLM%20Input.png)

### ã€LLM basicsã€‘LLM output
[![ã€LLM basicsã€‘LLM output](../images_english/png_small/%E3%80%90LLM%20basics%E3%80%91LLM%20output.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%E3%80%91LLM%20output.png)

### ã€LLM basicsã€‘MLLM and VLM
[![ã€LLM basicsã€‘MLLM and VLM](../images_english/png_small/%E3%80%90LLM%20basics%E3%80%91MLLM%20and%20VLM.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%E3%80%91MLLM%20and%20VLM.png)

### ã€LLM basicsã€‘LLM training process
[![ã€LLM basicsã€‘LLM training process](../images_english/png_small/%E3%80%90LLM%20basics%E3%80%91LLM%20training%20process.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%E3%80%91LLM%20training%20process.png)

### ã€SFTã€‘Categories of fine-tuning techniques
[![ã€SFTã€‘Categories of fine-tuning techniques](../images_english/png_small/%E3%80%90SFT%E3%80%91Categories%20of%20fine-tuning%20techniques.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91Categories%20of%20fine-tuning%20techniques.png)

### ã€SFTã€‘LoRA(1 of 2)
[![ã€SFTã€‘LoRA(1 of 2)](../images_english/png_small/%E3%80%90SFT%E3%80%91LoRA%281%20of%202%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91LoRA%281%20of%202%29.png)

### ã€SFTã€‘LoRA(2 of 2)
[![ã€SFTã€‘LoRA(2 of 2)](../images_english/png_small/%E3%80%90SFT%E3%80%91LoRA%282%20of%202%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91LoRA%282%20of%202%29.png)

### ã€SFTã€‘Prefix-Tuning
[![ã€SFTã€‘Prefix-Tuning](../images_english/png_small/%E3%80%90SFT%E3%80%91Prefix-Tuning.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91Prefix-Tuning.png)

### ã€SFTã€‘Token ID and Token
[![ã€SFTã€‘Token ID and Token](../images_english/png_small/%E3%80%90SFT%E3%80%91Token%20ID%20and%20Token.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91Token%20ID%20and%20Token.png)

### ã€SFTã€‘Loss of SFT(cross-entropy)
[![ã€SFTã€‘Loss of SFT(cross-entropy)](../images_english/png_small/%E3%80%90SFT%E3%80%91Loss%20of%20SFT%28cross-entropy%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91Loss%20of%20SFT%28cross-entropy%29.png)

### ã€SFTã€‘Packing of multiple pieces of sample
[![ã€SFTã€‘Packing of multiple pieces of sample](../images_english/png_small/%E3%80%90SFT%E3%80%91Packing%20of%20multiple%20pieces%20of%20sample.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90SFT%E3%80%91Packing%20of%20multiple%20pieces%20of%20sample.png)

### ã€DPOã€‘RLHF vs DPO
[![ã€DPOã€‘RLHF vs DPO](../images_english/png_small/%E3%80%90DPO%E3%80%91RLHF%20vs%20DPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90DPO%E3%80%91RLHF%20vs%20DPO.png)

### ã€DPOã€‘DPO(Direct Preference Optimization)
[![ã€DPOã€‘DPO(DirectPreferenceOptimization)](../images_english/png_small/%E3%80%90DPO%E3%80%91DPO%28DirectPreferenceOptimization%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90DPO%E3%80%91DPO%28DirectPreferenceOptimization%29.png)

### ã€DPOã€‘Overview of DPO training
[![ã€DPOã€‘Overview of DPO training](../images_english/png_small/%E3%80%90DPO%E3%80%91Overview%20of%20DPO%20training.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90DPO%E3%80%91Overview%20of%20DPO%20training.png)

### ã€DPOã€‘Impact of the Î² parameter on DPO
[![ã€DPOã€‘Impact of the Î² parameter on DPO](../images_english/png_small/%E3%80%90DPO%E3%80%91Impact%20of%20the%20%CE%B2%20parameter%20on%20DPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90DPO%E3%80%91Impact%20of%20the%20%CE%B2%20parameter%20on%20DPO.png)

### ã€DPOã€‘Effect of implicit reward differences on the magnitude of parameter updates
[![ã€DPOã€‘Effect of implicit reward differences on the magnitude of parameter updates](../images_english/png_small/%E3%80%90DPO%E3%80%91Effect%20of%20implicit%20reward%20differences%20on%20the%20magnitude%20of%20parameter%20updates.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90DPO%E3%80%91Effect%20of%20implicit%20reward%20differences%20on%20the%20magnitude%20of%20parameter%20updates.png)

### ã€Optimization without trainingã€‘Comparison of CoT and traditional Q&A
[![ã€Optimization without trainingã€‘Comparison of CoT and traditional Q&A](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Comparison%20of%20CoT%20and%20traditional%20Q%26A.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Comparison%20of%20CoT%20and%20traditional%20Q%26A.png)

### ã€Optimization without trainingã€‘CoTã€Self-consistency CoTã€ToTã€GoT <sup>[<a href="./references.md">87</a>]</sup>
[![ã€Optimization without trainingã€‘CoTã€Self-consistencyCoTã€ToTã€GoT](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91CoT%E3%80%81Self-consistencyCoT%E3%80%81ToT%E3%80%81GoT.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91CoT%E3%80%81Self-consistencyCoT%E3%80%81ToT%E3%80%81GoT.png)

### ã€Optimization without trainingã€‘Exhaustive Search
[![ã€Optimization without trainingã€‘Exhaustive Search](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Exhaustive%20Search.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Exhaustive%20Search.png)

### ã€Optimization without trainingã€‘Greedy Search
[![ã€Optimization without trainingã€‘Greedy Search](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Greedy%20Search.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Greedy%20Search.png)

### ã€Optimization without trainingã€‘Beam Search
[![ã€Optimization without trainingã€‘Beam Search](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Beam%20Search.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Beam%20Search.png)

### ã€Optimization without trainingã€‘Multinomial Sampling
[![ã€Optimization without trainingã€‘Multinomial Sampling](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Multinomial%20Sampling.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Multinomial%20Sampling.png)

### ã€Optimization without trainingã€‘Top-K Sampling
[![ã€Optimization without trainingã€‘Top-K Sampling](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Top-K%20Sampling.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Top-K%20Sampling.png)

### ã€Optimization without trainingã€‘Top-P Sampling
[![ã€Optimization without trainingã€‘Top-P Sampling](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Top-P%20Sampling.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Top-P%20Sampling.png)

### ã€Optimization without trainingã€‘RAG(Retrieval-Augmented Generation)
[![ã€Optimization without trainingã€‘RAG(Retrieval-Augmented Generation)](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91RAG%28Retrieval-Augmented%20Generation%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91RAG%28Retrieval-Augmented%20Generation%29.png)

### ã€Optimization without trainingã€‘Function Calling
[![ã€Optimization without trainingã€‘Function Calling](../images_english/png_small/%E3%80%90Optimization%20without%20training%E3%80%91Function%20Calling.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Optimization%20without%20training%E3%80%91Function%20Calling.png)

### ã€RL basicsã€‘History of RL
[![ã€RL basicsã€‘History of RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91History%20of%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91History%20of%20RL.png)

### ã€RL basicsã€‘Three major machine learning paradigms
[![ã€RL basicsã€‘Three major machine learning paradigms](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Three%20major%20machine%20learning%20paradigms.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Three%20major%20machine%20learning%20paradigms.png)

### ã€RL basicsã€‘Basic architecture of RL
[![ã€RL basicsã€‘Basic architecture of RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Basic%20architecture%20of%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Basic%20architecture%20of%20RL.png)

### ã€RL basicsã€‘Fundamental Concepts of RL
[![ã€RL basicsã€‘Fundamental Concepts of RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Fundamental%20Concepts%20of%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Fundamental%20Concepts%20of%20RL.png)

### ã€RL basicsã€‘Markov Chain vs MDP
[![ã€RL basicsã€‘Markov Chain vs MDP](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Markov%20Chain%20vs%20MDP.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Markov%20Chain%20vs%20MDP.png)

### ã€RL basicsã€‘Using dynamic Îµ values under the Îµ-greedy strategy
[![ã€RL basicsã€‘Using dynamic Îµ values under the Îµ-greedy strategy](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Using%20dynamic%20%CE%B5%20values%20under%20the%20%CE%B5-greedy%20strategy.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Using%20dynamic%20%CE%B5%20values%20under%20the%20%CE%B5-greedy%20strategy.png)

### ã€RL basicsã€‘Comparison of RL training paradigms
- On-policyï¼ŒOff-policyï¼ŒOffline RL
[![ã€RL basicsã€‘Comparison of RL training paradigms](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Comparison%20of%20RL%20training%20paradigms.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Comparison%20of%20RL%20training%20paradigms.png)

### ã€RL basicsã€‘Classification of RL
[![ã€RL basicsã€‘Classification of RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Classification%20of%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Classification%20of%20RL.png)

### ã€RL basicsã€‘Return(cumulative reward)
[![ã€RL basicsã€‘Return(cumulative reward)](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Return%28cumulative%20reward%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Return%28cumulative%20reward%29.png)

### ã€RL basicsã€‘Backwards iteration and computation of return G
[![ã€RL basicsã€‘Backwards iteration and computation of return G](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Backwards%20iteration%20and%20computation%20of%20return%20G.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Backwards%20iteration%20and%20computation%20of%20return%20G.png)

### ã€RL basicsã€‘Reward, Return, and Value
[![ã€RL basicsã€‘Reward, Return, and Value](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Reward%2C%20Return%2C%20and%20Value.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Reward%2C%20Return%2C%20and%20Value.png)

### ã€RL basicsã€‘QÏ€ and VÏ€
[![ã€RL basicsã€‘QÏ€ and VÏ€](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Q%CF%80%20and%20V%CF%80.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Q%CF%80%20and%20V%CF%80.png)

### ã€RL basicsã€‘Estimate the value through Monte Carlo(MC)
[![ã€RL basicsã€‘Estimate the value through Monte Carlo(MC)](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Estimate%20the%20value%20through%20Monte%20Carlo%28MC%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Estimate%20the%20value%20through%20Monte%20Carlo%28MC%29.png)

### ã€RL basicsã€‘TD target and TD error
[![ã€RL basicsã€‘TD target and TD error](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91TD%20target%20and%20TD%20error.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91TD%20target%20and%20TD%20error.png)

### ã€RL basicsã€‘TD(0), n-step TD, and MC
[![ã€RL basicsã€‘TD(0), n-step TD, and MC](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91TD%280%29%2C%20n-step%20TD%2C%20and%20MC.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91TD%280%29%2C%20n-step%20TD%2C%20and%20MC.png)

### ã€RL basicsã€‘Characteristics of MC and TD methods
[![ã€RL basicsã€‘Characteristics of MC and TD methods](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Characteristics%20of%20MC%20and%20TD%20methods.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Characteristics%20of%20MC%20and%20TD%20methods.png)

### ã€RL basicsã€‘MC, TD, DP, and exhaustive search <sup>[<a href="./references.md">32</a>]</sup>
[![ã€RL basicsã€‘MC, TD, DP, and exhaustive search](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91MC%2C%20TD%2C%20DP%2C%20and%20exhaustive%20search.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91MC%2C%20TD%2C%20DP%2C%20and%20exhaustive%20search.png)

### ã€RL basicsã€‘DQN model with two input-output structures
[![ã€RL basicsã€‘DQN model with two input-output structures](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91DQN%20model%20with%20two%20input-output%20structures.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91DQN%20model%20with%20two%20input-output%20structures.png)

### ã€RL basicsã€‘How to use DQN
[![ã€RL basicsã€‘How to use DQN](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91How%20to%20use%20DQN.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91How%20to%20use%20DQN.png)

### ã€RL basicsã€‘DQN's overestimation problem
[![ã€RL basicsã€‘DQN's overestimation problem](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91DQN%27s%20overestimation%20problem.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91DQN%27s%20overestimation%20problem.png)

### ã€RL basicsã€‘Value-Based vs Policy-Based
[![ã€RL basicsã€‘Value-Based vs Policy-Based](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Value-Based%20vs%20Policy-Based.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Value-Based%20vs%20Policy-Based.png)

### ã€RL basicsã€‘Policy gradient
[![ã€RL basicsã€‘Policy gradient](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Policy%20gradient.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Policy%20gradient.png)

### ã€RL basicsã€‘Multi-agent reinforcement learning(MARL)
[![ã€RL basicsã€‘Multi-agent reinforcement learning(MARL)](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Multi-agent%20reinforcement%20learning%28MARL%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Multi-agent%20reinforcement%20learning%28MARL%29.png)

### ã€RL basicsã€‘Multi-agent DDPG <sup>[<a href="./references.md">41</a>]</sup>
[![ã€RL basicsã€‘Multi-agent DDPG](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Multi-agent%20DDPG.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Multi-agent%20DDPG.png)

### ã€RL basicsã€‘Imitation learning(IL)
[![ã€RL basicsã€‘Imitation learning(IL)](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Imitation%20learning%28IL%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Imitation%20learning%28IL%29.png)

### ã€RL basicsã€‘Behavior cloning(BC)
[![ã€RL basicsã€‘Behavior cloning(BC)](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Behavior%20cloning%28BC%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Behavior%20cloning%28BC%29.png)

### ã€RL basicsã€‘Inverse RL(IRL) and RL
[![ã€RL basicsã€‘Inverse RL(IRL) and RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Inverse%20RL%28IRL%29%20and%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Inverse%20RL%28IRL%29%20and%20RL.png)

### ã€RL basicsã€‘Model-Based and Model-Free
[![ã€RL basicsã€‘Model-Based and Model-Free](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Model-Based%20and%20Model-Free.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Model-Based%20and%20Model-Free.png)

### ã€RL basicsã€‘Feudal RL
[![ã€RL basicsã€‘Feudal RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Feudal%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Feudal%20RL.png)

### ã€RL basicsã€‘Distributional RL
[![ã€RL basicsã€‘Distributional RL](../images_english/png_small/%E3%80%90RL%20basics%E3%80%91Distributional%20RL.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RL%20basics%E3%80%91Distributional%20RL.png)

### ã€Policy Optimization & Variantsã€‘Actor-Critic
[![ã€Policy Optimization & Variantsã€‘Actor-Critic](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Actor-Critic.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Actor-Critic.png)

### ã€Policy Optimization & Variantsã€‘Comparison of baseline and advantage
[![ã€Policy Optimization & Variantsã€‘Comparison of baseline and advantage](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Comparison%20of%20baseline%20and%20advantage.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Comparison%20of%20baseline%20and%20advantage.png)

### ã€Policy Optimization & Variantsã€‘GAE(Generalized Advantage Estimation)
[![ã€Policy Optimization & Variantsã€‘GAE](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91GAE.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91GAE.png)

### ã€Policy Optimization & Variantsã€‘TRPO and its trust region
[![ã€Policy Optimization & Variantsã€‘TRPO and its trust region](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91TRPO%20and%20its%20trust%20region.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91TRPO%20and%20its%20trust%20region.png)

### ã€Policy Optimization & Variantsã€‘Importance sampling
[![ã€Policy Optimization & Variantsã€‘Importance sampling](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Importance%20sampling.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Importance%20sampling.png)

### ã€Policy Optimization & Variantsã€‘PPO-Clip
[![ã€Policy Optimization & Variantsã€‘PPO-Clip](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91PPO-Clip.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91PPO-Clip.png)

### ã€Policy Optimization & Variantsã€‘Policy model update process in PPO training
[![ã€Policy Optimization & Variantsã€‘Policy model update process in PPO training](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Policy%20model%20update%20process%20in%20PPO%20training.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Policy%20model%20update%20process%20in%20PPO%20training.png)

### ã€Policy Optimization & Variantsã€‘GRPO & PPO <sup>[<a href="./references.md">72</a>]</sup>
[![ã€Policy Optimization & Variantsã€‘GRPO & PPO](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91GRPO%20%26%20PPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91GRPO%20%26%20PPO.png)

### ã€Policy Optimization & Variantsã€‘Deterministic policy vs. Stochastic policy
[![ã€Policy Optimization & Variantsã€‘Deterministic policy vs. Stochastic policy](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Deterministic%20policy%20vs.%20Stochastic%20policy.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91Deterministic%20policy%20vs.%20Stochastic%20policy.png)

### ã€Policy Optimization & Variantsã€‘DPG
[![ã€Policy Optimization & Variantsã€‘DPG](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91DPG.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91DPG.png)

### ã€Policy Optimization & Variantsã€‘DDPGï¼ˆDeep Deterministic Policy Gradientï¼‰
[![ã€Policy Optimization & Variantsã€‘DDPG](../images_english/png_small/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91DDPG.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Policy%20Optimization%20%26%20Variants%E3%80%91DDPG.png)

### ã€RLHF and RLAIFã€‘RL modeling of language models
[![ã€RLHF and RLAIFã€‘RL modeling of language models](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91RL%20modeling%20of%20language%20models.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91RL%20modeling%20of%20language%20models.png)

### ã€RLHF and RLAIFã€‘Two-stage training process of RLHF
[![ã€RLHF and RLAIFã€‘Two-stage training process of RLHF](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Two-stage%20training%20process%20of%20RLHF.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Two-stage%20training%20process%20of%20RLHF.png)

### ã€RLHF and RLAIFã€‘Structure of the reward model
[![ã€RLHF and RLAIFã€‘Structure of the reward model](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Structure%20of%20the%20reward%20model.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Structure%20of%20the%20reward%20model.png)

### ã€RLHF and RLAIFã€‘Input and output of the reward model
[![ã€RLHF and RLAIFã€‘Input and output of the reward model](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Input%20and%20output%20of%20the%20reward%20model.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Input%20and%20output%20of%20the%20reward%20model.png)

### ã€RLHF and RLAIFã€‘Reward deviation and loss
[![ã€RLHF and RLAIFã€‘Reward deviation and loss](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Reward%20deviation%20and%20loss.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Reward%20deviation%20and%20loss.png)

### ã€RLHF and RLAIFã€‘Training of the reward model
[![ã€RLHF and RLAIFã€‘Training of the reward model](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Training%20of%20the%20reward%20model.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Training%20of%20the%20reward%20model.png)

### ã€RLHF and RLAIFã€‘Relationship between the four models in PPO
[![ã€RLHF and RLAIFã€‘Relationship between the four models in PPO](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Relationship%20between%20the%20four%20models%20in%20PPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Relationship%20between%20the%20four%20models%20in%20PPO.png)

### ã€RLHF and RLAIFã€‘The structure and init of the four models in PPO
[![ã€RLHF and RLAIFã€‘The structure and init of the four models in PPO](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91The%20structure%20and%20init%20of%20the%20four%20models%20in%20PPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91The%20structure%20and%20init%20of%20the%20four%20models%20in%20PPO.png)

### ã€RLHF and RLAIFã€‘A value model with a dual-head structure
[![ã€RLHF and RLAIFã€‘A value model with a dual-head structure](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91A%20value%20model%20with%20a%20dual-head%20structure.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91A%20value%20model%20with%20a%20dual-head%20structure.png)

### ã€RLHF and RLAIFã€‘Four models can share one base in RLHF
[![ã€RLHF and RLAIFã€‘Four models can share one base in RLHF](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Four%20models%20can%20share%20one%20base%20in%20RLHF.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Four%20models%20can%20share%20one%20base%20in%20RLHF.png)

### ã€RLHF and RLAIFã€‘Inputs and Outputs of Each Model in PPO
[![ã€RLHF and RLAIFã€‘Inputs and Outputs of Each Model in PPO](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Inputs%20and%20Outputs%20of%20Each%20Model%20in%20PPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Inputs%20and%20Outputs%20of%20Each%20Model%20in%20PPO.png)

### ã€RLHF and RLAIFã€‘The Process of Calculating KL in PPO
[![ã€RLHF and RLAIFã€‘The Process of Calculating KL in PPO](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91The%20Process%20of%20Calculating%20KL%20in%20PPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91The%20Process%20of%20Calculating%20KL%20in%20PPO.png)

### ã€RLHF and RLAIFã€‘RLHF Training Based on PPO
[![ã€RLHF and RLAIFã€‘RLHF Training Based on PPO](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91RLHF%20Training%20Based%20on%20PPO.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91RLHF%20Training%20Based%20on%20PPO.png)

### ã€RLHF and RLAIFã€‘Rejection Sampling Fine-tuning
[![ã€RLHF and RLAIFã€‘Rejection Sampling Fine-tuning](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Rejection%20Sampling%20Fine-tuning.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91Rejection%20Sampling%20Fine-tuning.png)

### ã€RLHF and RLAIFã€‘RLAIF vs RLHF
[![ã€RLHF and RLAIFã€‘RLAIF vs RLHF](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91RLAIF%20vs%20RLHF.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91RLAIF%20vs%20RLHF.png)

### ã€RLHF and RLAIFã€‘CAI(Constitutional AI)
[![ã€RLHF and RLAIFã€‘CAI(Constitutional AI)](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91CAI%28Constitutional%20AI%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91CAI%28Constitutional%20AI%29.png)

### ã€RLHF and RLAIFã€‘OpenAI RBR(Rule-Based Reward)
[![ã€RLHF and RLAIFã€‘OpenAI RBR(Rule-Based Reward)](../images_english/png_small/%E3%80%90RLHF%20and%20RLAIF%E3%80%91OpenAI%20RBR%28Rule-Based%20Reward%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90RLHF%20and%20RLAIF%E3%80%91OpenAI%20RBR%28Rule-Based%20Reward%29.png)

### ã€Reasoning capacity optimizationã€‘Knowledge Distillation Based on CoT
[![ã€Reasoning capacity optimizationã€‘Knowledge Distillation Based on CoT](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Knowledge%20Distillation%20Based%20on%20CoT.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Knowledge%20Distillation%20Based%20on%20CoT.png)

### ã€Reasoning capacity optimizationã€‘Distillation Based on DeepSeek
[![ã€Reasoning capacity optimizationã€‘Distillation Based on DeepSeek](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Distillation%20Based%20on%20DeepSeek.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Distillation%20Based%20on%20DeepSeek.png)

### ã€Reasoning capacity optimizationã€‘ORM(Outcome Reward Model)Â &Â PRM (Process Reward Model)
[![ã€Reasoning capacity optimizationã€‘ORMÂ &Â PRM](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91ORM%C2%A0%26%C2%A0PRM.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91ORM%C2%A0%26%C2%A0PRM.png)

### ã€Reasoning capacity optimizationã€‘Four Key Steps of Each MCTS
[![ã€Reasoning capacity optimizationã€‘Four Key Steps of Each MCTS](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Four%20Key%20Steps%20of%20Each%20MCTS.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Four%20Key%20Steps%20of%20Each%20MCTS.png)

### ã€Reasoning capacity optimizationã€‘MCTS
[![ã€Reasoning capacity optimizationã€‘MCTS](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91MCTS.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91MCTS.png)

### ã€Reasoning capacity optimizationã€‘Search Tree Example in a Linguistic Context
[![ã€Reasoning capacity optimizationã€‘Search Tree Example in a Linguistic Context](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Search%20Tree%20Example%20in%20a%20Linguistic%20Context.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Search%20Tree%20Example%20in%20a%20Linguistic%20Context.png)

### ã€Reasoning capacity optimizationã€‘BoN(Best-of-N) Sampling
[![ã€Reasoning capacity optimizationã€‘BoN(Best-of-N) Sampling](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91BoN%28Best-of-N%29%20Sampling.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91BoN%28Best-of-N%29%20Sampling.png)

### ã€Reasoning capacity optimizationã€‘Majority Vote
[![ã€Reasoning capacity optimizationã€‘Majority Vote](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Majority%20Vote.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Majority%20Vote.png)

### ã€Reasoning capacity optimizationã€‘Performance Growth of AlphaGo Zero <sup>[<a href="./references.md">179</a>]</sup>
[![ã€Reasoning capacity optimizationã€‘Performance Growth of AlphaGo Zero](../images_english/png_small/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Performance%20Growth%20of%20AlphaGo%20Zero.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90Reasoning%20capacity%20optimization%E3%80%91Performance%20Growth%20of%20AlphaGo%20Zero.png)

### ã€LLM basics extendedã€‘Performance Optimization Map for Large Models
[![ã€LLM basics extendedã€‘Performance Optimization Map for Large Models](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Performance%20Optimization%20Map%20for%20Large%20Models.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Performance%20Optimization%20Map%20for%20Large%20Models.png)

### ã€LLM basics extendedã€‘ALiBi positional encoding
[![ã€LLM basics extendedã€‘ALiBi positional encoding](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91ALiBi%20positional%20encoding.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91ALiBi%20positional%20encoding.png)

### ã€LLM basics extendedã€‘Traditional knowledge distillation
[![ã€LLM basics extendedã€‘Traditional knowledge distillation](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Traditional%20knowledge%20distillation.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Traditional%20knowledge%20distillation.png)

### ã€LLM basics extendedã€‘Numerical representation, quantization
[![ã€LLM basics extendedã€‘Numerical representation, quantization](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Numerical%20representation%2C%20quantization.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Numerical%20representation%2C%20quantization.png)

### ã€LLM basics extendedã€‘Forward and backward
[![ã€LLM basics extendedã€‘Forward and backward](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Forward%20and%20backward.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Forward%20and%20backward.png)

### ã€LLM basics extendedã€‘Gradient Accumulation
[![ã€LLM basics extendedã€‘Gradient Accumulation](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Gradient%20Accumulation.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Gradient%20Accumulation.png)

### ã€LLM basics extendedã€‘Gradient Checkpoint(gradient recomputation)
[![ã€LLM basics extendedã€‘Gradient Checkpoint(gradient recomputation)](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Gradient%20Checkpoint%28gradient%20recomputation%29.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Gradient%20Checkpoint%28gradient%20recomputation%29.png)

### ã€LLM basics extendedã€‘Full recomputation 
[![ã€LLM basics extendedã€‘Full recomputation ](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Full%20recomputation%20.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Full%20recomputation%20.png)

### ã€LLM basics extendedã€‘LLM Benchmark
[![ã€LLM basics extendedã€‘LLM Benchmark](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91LLM%20Benchmark.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91LLM%20Benchmark.png)

### ã€LLM basics extendedã€‘MHAã€GQAã€MQAã€MLA
[![ã€LLM basics extendedã€‘MHAã€GQAã€MQAã€MLA](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91MHA%E3%80%81GQA%E3%80%81MQA%E3%80%81MLA.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91MHA%E3%80%81GQA%E3%80%81MQA%E3%80%81MLA.png)

### ã€LLM basics extendedã€‘RNN(Recurrent Neural Network)
[![ã€LLM basics extendedã€‘RNN](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91RNN.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91RNN.png)

### ã€LLM basics extendedã€‘Pre-norm vs Post-norm
[![ã€LLM basics extendedã€‘Pre-norm vs Post-norm](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Pre-norm%20vs%20Post-norm.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Pre-norm%20vs%20Post-norm.png)

### ã€LLM basics extendedã€‘BatchNormÂ &Â LayerNorm
[![ã€LLM basics extendedã€‘BatchNormÂ &Â LayerNorm](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91BatchNorm%C2%A0%26%C2%A0LayerNorm.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91BatchNorm%C2%A0%26%C2%A0LayerNorm.png)

### ã€LLM basics extendedã€‘RMSNorm
[![ã€LLM basics extendedã€‘RMSNorm](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91RMSNorm.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91RMSNorm.png)

### ã€LLM basics extendedã€‘Prune
[![ã€LLM basics extendedã€‘Prune](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Prune.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Prune.png)

### ã€LLM basics extendedã€‘Role of the temperature coefficient
[![ã€LLM basics extendedã€‘Role of the temperature coefficient](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91Role%20of%20the%20temperature%20coefficient.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91Role%20of%20the%20temperature%20coefficient.png)

### ã€LLM basics extendedã€‘SwiGLU
[![ã€LLM basics extendedã€‘SwiGLU](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91SwiGLU.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91SwiGLU.png)

### ã€LLM basics extendedã€‘AUCã€PRã€F1ã€Precisionã€Recall
[![ã€LLM basics extendedã€‘AUCã€PRã€F1ã€Precisionã€Recall](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91AUC%E3%80%81PR%E3%80%81F1%E3%80%81Precision%E3%80%81Recall.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91AUC%E3%80%81PR%E3%80%81F1%E3%80%81Precision%E3%80%81Recall.png)

### ã€LLM basics extendedã€‘RoPE positional encoding
[![ã€LLM basics extendedã€‘RoPE positional encoding](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91RoPE%20positional%20encoding.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91RoPE%20positional%20encoding.png)

### ã€LLM basics extendedã€‘The effect of RoPE on each sequence position and each dim
- For details on the principles of RoPE, the base and Î¸ values, and how they work, see: [RoPE-theta-base.xlsx](./RoPE-theta-base.xlsx) 


[![ã€LLM basics extendedã€‘The effect of RoPE on each sequence position and each dim](../images_english/png_small/%E3%80%90LLM%20basics%20extended%E3%80%91The%20effect%20of%20RoPE%20on%20each%20sequence%20position%20and%20each%20dim.png)](https://raw.githubusercontent.com/changyeyu/LLM-RL-Visualized/master/images_english/png_big/%E3%80%90LLM%20basics%20extended%E3%80%91The%20effect%20of%20RoPE%20on%20each%20sequence%20position%20and%20each%20dim.png)



<br>

---


## Contributing
- **Contributions are welcome!** Whether it's new diagrams, documentation, error corrections, or other improvements. You're welcome to include your name or nickname in the diagrams. Your GitHub account will also be listed in the **Contributors** to help more people discover you and your work. Example diagram template: [images-template.pptx](./assets/images-template.pptx)


-  **How to contribute:**  
  (1) Fork: Click the "Fork" button to create a copy of the repo under your GitHub account â†’  
  (2) Clone: Clone the forked repo to your local environment â†’  
  (3) Create a new local branch â†’  
  (4) Make changes and commit â†’  
  (5) Push changes to your remote repo â†’  
  (6) Submit a PR: On GitHub, go to your forked repo and click "Compare & pull request" to submit a PR. The maintainer will review and merge it into the main repository.

-  **Suggested color** scheme for diagram design:  
  <span style="display:inline-block;width:12px;height:12px;background-color:#71CCF5;border-radius:2px;margin-right:8px;"></span>Light Blue (`#71CCF5`) ;  
  <span style="display:inline-block;width:12px;height:12px;background-color:#FFE699;border-radius:2px;margin-right:8px;"></span>Light Yellow (`#FFE699`) ;  
  <span style="display:inline-block;width:12px;height:12px;background-color:#C0BFDE;border-radius:2px;margin-right:8px;"></span>Blue-Purple (`#C0BFDE`) ;  
  <span style="display:inline-block;width:12px;height:12px;background-color:#F0ADB7;border-radius:2px;margin-right:8px;"></span>Pink (`#F0ADB7`)


## Terms of Use
All images in this repository are licensed under [LICENSE](../LICENSE). You are free to use, modify, and remix the materials under the following terms:  
- **Sharing** â€” You may copy and redistribute the material in any format.  
- **Adapting** â€” You may remix, transform, and build upon the material.

You must also comply with the following terms:  
- **For Web Use** â€” If using the materials in blog posts or online content, please retain the original author information embedded in the images.  
- **For Papers, Books, and Publications** â€” If using the materials in formal publications, please cite the source using the format below. In such cases, the embedded author info may be removed from the image.  
- **Non-commercial Use Only** â€” These materials may not be used for any direct commercial purposes.



## Citation

If you use any content from this project (including diagrams or concepts from the book), please cite it as follows:

#### ğŸ“Œ For Reference Section
```
Yu, Changye. Large Model Algorithms: Reinforcement Learning, Fine-Tuning, and Alignment. 
Beijing: Publishing House of Electronics Industry, 2025. https://github.com/changyeyu/LLM-RL-Visualized
```

#### ğŸ“Œ BibTeX Citation Format
```bibtex
@book{yu2025largemodel_en,
  title     = {Large Model Algorithms: Reinforcement Learning, Fine-Tuning, and Alignment},
  author    = {Yu, Changye},
  publisher = {Publishing House of Electronics Industry},
  year      = {2025},
  address   = {Beijing},
  isbn      = {9787121500725},
  url       = {https://github.com/changyeyu/LLM-RL-Visualized},
  language  = {en}
}
```

---

<div align="center"> Continuously <strong> updating</strong>...   Click â­<strong>Star</strong> at the top-right to follow! </div> 